{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPn5iSMacRYak/KzbgV3CjZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suhana-09-2005/NLP/blob/main/NLP_assignment_2_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9Xrxr8OY-0x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b00f52e"
      },
      "source": [
        "# Task\n",
        "Load a medical text corpus, then tokenize it into sentences and words using NLTK and spaCy. Apply stemming using an NLTK stemmer and lemmatization using spaCy's lemmatizer, displaying the original words and their processed forms. Finally, compare the outputs of stemming and lemmatization, and discuss why lemmatization is crucial for accuracy and meaning preservation in healthcare Natural Language Processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfdbd4c6"
      },
      "source": [
        "## Load Medical Text Corpus\n",
        "\n",
        "### Subtask:\n",
        "Load the sensitive medical or healthcare-related text corpus from a source (e.g., a string or file) for preprocessing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93ae0419"
      },
      "source": [
        "**Reasoning**:\n",
        "To fulfill the first instruction, I will define a variable `medical_text` and assign a sample medical/healthcare-related multi-line string to it, including various medical terms, symptoms, treatments, and patient information for later preprocessing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31b45636",
        "outputId": "85f9c562-fd3b-4419-e9ee-c66f6b51a432"
      },
      "source": [
        "medical_text = \"\"\"Patient Name: John Doe\\nDOB: 1985-03-15\\nMedical Record Number: 789012\\n\\nDiagnosis: Acute appendicitis. Patient presented with severe abdominal pain, localized in the right lower quadrant, nausea, and vomiting. Temperature was 100.5°F (38.1°C), and white blood cell count was elevated at 15,000/µL.\\n\\nTreatment: Surgical appendectomy performed on 2023-10-26. Post-operative recovery was uneventful. Administered IV antibiotics (Ceftriaxone) for 48 hours. Patient discharged on 2023-10-28 with instructions to take oral analgesics (Ibuprofen) as needed and to follow up in 2 weeks.\\n\\nMedications: Ceftriaxone (discontinued), Ibuprofen (as needed).\\nAllergies: Penicillin (rash).\\n\\nNotes: Patient advised on wound care and signs of infection. Emphasized importance of hydration and light activity. Follow-up appointment scheduled with Dr. Smith.\"\"\"\n",
        "\n",
        "print(\"Medical text loaded successfully:\")\n",
        "print(medical_text[:200] + \"...\") # Print a snippet to verify"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medical text loaded successfully:\n",
            "Patient Name: John Doe\n",
            "DOB: 1985-03-15\n",
            "Medical Record Number: 789012\n",
            "\n",
            "Diagnosis: Acute appendicitis. Patient presented with severe abdominal pain, localized in the right lower quadrant, nausea, and vo...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "789563f9"
      },
      "source": [
        "## Tokenize Sentences and Words\n",
        "\n",
        "### Subtask:\n",
        "Tokenize the loaded medical text into sentences and then into words using both NLTK and spaCy libraries. This will prepare the text for further linguistic analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2487483c"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction requires installing NLTK if it's not already installed. This ensures that the necessary library is available for tokenization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "909e5852",
        "outputId": "a1d54992-5b7b-4ba8-9f74-02d8601464ce"
      },
      "source": [
        "try:\n",
        "    import nltk\n",
        "    print(\"NLTK is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"NLTK not found. Installing NLTK...\")\n",
        "    !pip install nltk\n",
        "    import nltk\n",
        "    print(\"NLTK installed successfully.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK is already installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a52c06b"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to ensure spaCy is installed, as per instruction 1, to prepare for tokenization using the spaCy library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d2475e5",
        "outputId": "9eb0b9cb-aefa-4bcd-a430-39eb557d7e37"
      },
      "source": [
        "try:\n",
        "    import spacy\n",
        "    print(\"spaCy is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"spaCy not found. Installing spaCy...\")\n",
        "    !pip install spacy\n",
        "    import spacy\n",
        "    print(\"spaCy installed successfully.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy is already installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36b0af6c"
      },
      "source": [
        "**Reasoning**:\n",
        "As per instruction 2, I will download the 'punkt' tokenizer models required by NLTK for sentence and word tokenization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12dbf252",
        "outputId": "84fd9e93-baeb-420e-f9de-74700293ded5"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "print(\"NLTK 'punkt' data downloaded successfully.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK 'punkt' data downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8c0315e"
      },
      "source": [
        "**Reasoning**:\n",
        "As per instruction 3, I will download the 'en_core_web_sm' English model for spaCy, which is necessary for spaCy's tokenization and other linguistic processing capabilities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f39b9a19",
        "outputId": "b61c9706-5826-4aca-c0b1-5f4619f89bdc"
      },
      "source": [
        "import spacy\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    print(\"spaCy 'en_core_web_sm' model already loaded.\")\n",
        "except OSError:\n",
        "    print(\"spaCy 'en_core_web_sm' model not found. Downloading...\")\n",
        "    !python -m spacy download en_core_web_sm\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    print(\"spaCy 'en_core_web_sm' model downloaded and loaded successfully.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy 'en_core_web_sm' model already loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2c862c4"
      },
      "source": [
        "**Reasoning**:\n",
        "Following instruction 4, I will import the necessary NLTK tokenization functions (`sent_tokenize` and `word_tokenize`) to prepare for tokenizing the medical text into sentences and words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4d17741",
        "outputId": "e8cb8243-bed9-4250-e596-9d73998d243a"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "print(\"NLTK tokenization functions imported successfully.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK tokenization functions imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8609e0f7"
      },
      "source": [
        "**Reasoning**:\n",
        "Following instruction 6, I will use `nltk.sent_tokenize()` on the `medical_text` to tokenize it into sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08154daf",
        "outputId": "9fafe7ee-fa51-46e7-d58b-b46aef0fbdcc"
      },
      "source": [
        "# Import required libraries\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Download required tokenizer (run once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Input medical text\n",
        "medical_text = (\n",
        "    \"Diabetes is a chronic disease. It affects how the body processes blood sugar. \"\n",
        "    \"Early diagnosis helps in better management. Regular checkups are important.\"\n",
        ")\n",
        "\n",
        "# Sentence tokenization\n",
        "nltk_sentences = sent_tokenize(medical_text)\n",
        "\n",
        "# Output\n",
        "print(f\"NLTK tokenized {len(nltk_sentences)} sentences. First 3 sentences:\\n{nltk_sentences[:3]}\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK tokenized 4 sentences. First 3 sentences:\n",
            "['Diabetes is a chronic disease.', 'It affects how the body processes blood sugar.', 'Early diagnosis helps in better management.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b18eb89"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous cell failed because the `punkt_tab` resource, which is required by NLTK's `PunktTokenizer`, was not found. Although 'punkt' was downloaded, the traceback suggests 'punkt_tab' is a separate or more specific resource. I will explicitly download `punkt_tab` to resolve this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e551eafc",
        "outputId": "b1ff74f8-2b58-46d0-d086-17632a228fc6"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Explicitly download punkt_tab\n",
        "print(\"NLTK 'punkt' and 'punkt_tab' data downloaded successfully.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK 'punkt' and 'punkt_tab' data downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2637a00"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the `punkt_tab` resource has now been successfully downloaded, I will re-attempt NLTK sentence tokenization as per instruction 6 and then proceed with NLTK word tokenization as per instruction 7, which depends on the sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b01e7827",
        "outputId": "63afa5e6-9635-4312-cd6c-5865d04bad8f"
      },
      "source": [
        "nltk_sentences = sent_tokenize(medical_text)\n",
        "nltk_words_per_sentence = [word_tokenize(sentence) for sentence in nltk_sentences]\n",
        "\n",
        "print(f\"NLTK tokenized {len(nltk_sentences)} sentences. First 3 sentences:\\n{nltk_sentences[:3]}\\n\")\n",
        "print(f\"NLTK tokenized words for the first sentence:\\n{nltk_words_per_sentence[0][:10]}...\") # Display first 10 words of the first sentence"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK tokenized 12 sentences. First 3 sentences:\n",
            "['Patient Name: John Doe\\nDOB: 1985-03-15\\nMedical Record Number: 789012\\n\\nDiagnosis: Acute appendicitis.', 'Patient presented with severe abdominal pain, localized in the right lower quadrant, nausea, and vomiting.', 'Temperature was 100.5°F (38.1°C), and white blood cell count was elevated at 15,000/µL.']\n",
            "\n",
            "NLTK tokenized words for the first sentence:\n",
            "['Patient', 'Name', ':', 'John', 'Doe', 'DOB', ':', '1985-03-15', 'Medical', 'Record']...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "071ccb44"
      },
      "source": [
        "**Reasoning**:\n",
        "Following instruction 8, I will process the `medical_text` with the loaded spaCy model (`nlp`) to create a spaCy `Doc` object. This `Doc` object will then be used for spaCy-based sentence and word tokenization in subsequent steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adadcf01",
        "outputId": "f51572e2-394e-42a1-8cf3-7d535d466185"
      },
      "source": [
        "spacy_doc = nlp(medical_text)\n",
        "print(\"Medical text processed with spaCy NLP model successfully.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medical text processed with spaCy NLP model successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9023b588"
      },
      "source": [
        "**Reasoning**:\n",
        "Following instruction 9, I will extract sentences from the `spacy_doc` object using `doc.sents` to get spaCy's sentence tokenization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2907230",
        "outputId": "150cfb4b-64b8-425c-eb8b-8715cd257e9c"
      },
      "source": [
        "spacy_sentences = [sent.text for sent in spacy_doc.sents]\n",
        "print(f\"spaCy tokenized {len(spacy_sentences)} sentences. First 3 sentences:\\n{spacy_sentences[:3]}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy tokenized 11 sentences. First 3 sentences:\n",
            "['Patient Name: John Doe\\nDOB: 1985-03-15\\nMedical Record Number: 789012\\n\\nDiagnosis: Acute appendicitis.', 'Patient presented with severe abdominal pain, localized in the right lower quadrant, nausea, and vomiting.', 'Temperature was 100.5°F (38.1°C), and white blood cell count was elevated at 15,000/µL.\\n\\nTreatment: Surgical appendectomy performed on 2023-10-26.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddd768a5"
      },
      "source": [
        "**Reasoning**:\n",
        "Following instruction 10, I will extract spaCy tokens (words) from the `spacy_doc` object. This will give a flat list of all words/tokens identified by spaCy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b679b12",
        "outputId": "53f1a112-9e3c-4f32-d05e-6e276097717c"
      },
      "source": [
        "# Import spaCy\n",
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input text\n",
        "text = \"spaCy is an open-source library used for Natural Language Processing tasks.\"\n",
        "\n",
        "# Process text\n",
        "spacy_doc = nlp(text)\n",
        "\n",
        "# Word tokenization\n",
        "spacy_words = [token.text for token in spacy_doc]\n",
        "\n",
        "# Correct print statement (use \\n)\n",
        "print(\n",
        "    f\"spaCy tokenized {len(spacy_words)} words. \"\n",
        "    f\"First 10 words:\\n{spacy_words[:10]}\"\n",
        ")\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy tokenized 14 words. First 10 words:\n",
            "['spaCy', 'is', 'an', 'open', '-', 'source', 'library', 'used', 'for', 'Natural']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df430618"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed due to a `SyntaxError: unterminated f-string literal`. I will fix the f-string by ensuring the entire string is on a single line to correctly display the spaCy tokenized words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "037b3d1a",
        "outputId": "bd69f0aa-fa74-40ed-d199-35b1779d3f0b"
      },
      "source": [
        "spacy_words = [token.text for token in spacy_doc]\n",
        "print(f\"spaCy tokenized {len(spacy_words)} words. First 10 words: {spacy_words[:10]}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy tokenized 173 words. First 10 words: ['Patient', 'Name', ':', 'John', 'Doe', '\\n', 'DOB', ':', '1985', '-']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82d40b8b"
      },
      "source": [
        "## Apply Stemming\n",
        "\n",
        "### Subtask:\n",
        "Apply stemming to the tokenized medical terminology using an appropriate NLTK stemmer (e.g., PorterStemmer or SnowballStemmer). Display the original words and their stemmed forms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b004fb8"
      },
      "source": [
        "**Reasoning**:\n",
        "As per instruction 1, I will import the `PorterStemmer` class from `nltk.stem` to prepare for stemming the NLTK tokenized words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22405221",
        "outputId": "f76d3472-b9f5-46fc-d46c-a8cc2c7e5ce6"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "print(\"PorterStemmer imported successfully.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PorterStemmer imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a88e2bcb"
      },
      "source": [
        "**Reasoning**:\n",
        "Following instruction 2, I will instantiate the `PorterStemmer`. Then, as per instructions 3, 4, and 5, I will initialize an empty list `stemmed_words_nltk` and iterate through `nltk_words_per_sentence` to apply the stemmer to each word and store the original and stemmed forms. Finally, as per instruction 6, I will print a sample of the first 20 original and stemmed words to verify the output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f86790a",
        "outputId": "eabf2351-6654-4bcb-cb2a-c0829c4d6847"
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_words_nltk = []\n",
        "\n",
        "for sentence_words in nltk_words_per_sentence:\n",
        "    for word in sentence_words:\n",
        "        stemmed_word = stemmer.stem(word)\n",
        "        stemmed_words_nltk.append((word, stemmed_word))\n",
        "\n",
        "print(f\"Total unique original words stemmed: {len(set([word for word, _ in stemmed_words_nltk]))}\")\n",
        "print(\"Sample of original and stemmed words (first 20):\")\n",
        "for original, stemmed in stemmed_words_nltk[:20]:\n",
        "    print(f\"Original: {original}, Stemmed: {stemmed}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique original words stemmed: 95\n",
            "Sample of original and stemmed words (first 20):\n",
            "Original: Patient, Stemmed: patient\n",
            "Original: Name, Stemmed: name\n",
            "Original: :, Stemmed: :\n",
            "Original: John, Stemmed: john\n",
            "Original: Doe, Stemmed: doe\n",
            "Original: DOB, Stemmed: dob\n",
            "Original: :, Stemmed: :\n",
            "Original: 1985-03-15, Stemmed: 1985-03-15\n",
            "Original: Medical, Stemmed: medic\n",
            "Original: Record, Stemmed: record\n",
            "Original: Number, Stemmed: number\n",
            "Original: :, Stemmed: :\n",
            "Original: 789012, Stemmed: 789012\n",
            "Original: Diagnosis, Stemmed: diagnosi\n",
            "Original: :, Stemmed: :\n",
            "Original: Acute, Stemmed: acut\n",
            "Original: appendicitis, Stemmed: append\n",
            "Original: ., Stemmed: .\n",
            "Original: Patient, Stemmed: patient\n",
            "Original: presented, Stemmed: present\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1253fb3f"
      },
      "source": [
        "## Apply Lemmatization\n",
        "\n",
        "### Subtask:\n",
        "Apply lemmatization to the tokenized medical terminology using spaCy's lemmatizer. Display the original words and their lemmatized forms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b4c9156"
      },
      "source": [
        "**Reasoning**:\n",
        "As per instruction 1, 2, 3, and 4, I will initialize an empty list `lemmatized_words_spacy`, iterate through each token in the `spacy_doc` object, extract its original text and lemmatized form, and append them as a tuple to the list. Then, following instruction 5 and 6, I will print the total count of unique original words lemmatized and display a sample of the first 20 original and lemmatized words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56a6bcc4",
        "outputId": "7f3d7047-f8a3-4ecf-cd0d-7b9c0497a79c"
      },
      "source": [
        "lemmatized_words_spacy = []\n",
        "\n",
        "for token in spacy_doc:\n",
        "    lemmatized_words_spacy.append((token.text, token.lemma_))\n",
        "\n",
        "print(f\"Total unique original words lemmatized: {len(set([word for word, _ in lemmatized_words_spacy]))}\")\n",
        "print(\"Sample of original and lemmatized words (first 20):\")\n",
        "for original, lemmatized in lemmatized_words_spacy[:20]:\n",
        "    print(f\"Original: {original}, Lemmatized: {lemmatized}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique original words lemmatized: 106\n",
            "Sample of original and lemmatized words (first 20):\n",
            "Original: Patient, Lemmatized: Patient\n",
            "Original: Name, Lemmatized: Name\n",
            "Original: :, Lemmatized: :\n",
            "Original: John, Lemmatized: John\n",
            "Original: Doe, Lemmatized: Doe\n",
            "Original: \n",
            ", Lemmatized: \n",
            "\n",
            "Original: DOB, Lemmatized: DOB\n",
            "Original: :, Lemmatized: :\n",
            "Original: 1985, Lemmatized: 1985\n",
            "Original: -, Lemmatized: -\n",
            "Original: 03, Lemmatized: 03\n",
            "Original: -, Lemmatized: -\n",
            "Original: 15, Lemmatized: 15\n",
            "Original: \n",
            ", Lemmatized: \n",
            "\n",
            "Original: Medical, Lemmatized: Medical\n",
            "Original: Record, Lemmatized: Record\n",
            "Original: Number, Lemmatized: Number\n",
            "Original: :, Lemmatized: :\n",
            "Original: 789012, Lemmatized: 789012\n",
            "Original: \n",
            "\n",
            ", Lemmatized: \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab99bce6"
      },
      "source": [
        "## Compare Outputs and Discuss\n",
        "\n",
        "### Subtask:\n",
        "Compare the outputs of stemming and lemmatization. Provide a detailed discussion and explanation of why lemmatization is critical in healthcare Natural Language Processing, considering its impact on accuracy and meaning preservation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26128283"
      },
      "source": [
        "**Reasoning**:\n",
        "As per instruction 1, I will define a list of medical terms to compare. Then, I will iterate through these terms, applying both the NLTK Porter Stemmer and spaCy's lemmatizer to each word. Finally, I will display the original, stemmed, and lemmatized forms in a structured way to illustrate their differences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "911fd9e1",
        "outputId": "250a2124-f36e-443a-d07f-ebf1714306c4"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "comparison_words = ['diagnosed', 'diagnoses', 'imaging', 'images', 'treatments', 'treated', 'recovery', 'recovered', 'antibiotics', 'antibiotic', 'cells', 'cell', 'presented', 'presenting', 'localization', 'localized']\n",
        "\n",
        "comparison_results = []\n",
        "\n",
        "for word in comparison_words:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    # For spaCy lemmatization, process the word as a mini-document\n",
        "    doc = nlp(word)\n",
        "    # Check if doc has at least one token before accessing doc[0]\n",
        "    lemmatized_word = doc[0].lemma_ if doc else word\n",
        "    comparison_results.append({\"Original\": word, \"Stemmed\": stemmed_word, \"Lemmatized\": lemmatized_word})\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_results)\n",
        "print(\"Comparison of Stemming and Lemmatization for Medical Terms:\")\n",
        "print(comparison_df.to_string())\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison of Stemming and Lemmatization for Medical Terms:\n",
            "        Original    Stemmed    Lemmatized\n",
            "0      diagnosed    diagnos      diagnose\n",
            "1      diagnoses    diagnos      diagnose\n",
            "2        imaging       imag         image\n",
            "3         images       imag         image\n",
            "4     treatments  treatment     treatment\n",
            "5        treated      treat         treat\n",
            "6       recovery   recoveri      recovery\n",
            "7      recovered      recov       recover\n",
            "8    antibiotics   antibiot    antibiotic\n",
            "9     antibiotic   antibiot    antibiotic\n",
            "10         cells       cell          cell\n",
            "11          cell       cell          cell\n",
            "12     presented    present       present\n",
            "13    presenting    present       present\n",
            "14  localization      local  localization\n",
            "15     localized      local      localize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd7b51d1"
      },
      "source": [
        "### Comparison of Stemming and Lemmatization and their Importance in Healthcare NLP\n",
        "\n",
        "**1. What each technique does:**\n",
        "\n",
        "*   **Stemming:** Stemming is a heuristic process that chops off the ends of words in the hope of achieving a common base form, often called a 'stem'. It typically removes suffixes to reduce words to a common root form, but this root form is not necessarily a valid word. For example, the Porter Stemmer might reduce 'diagnosed' and 'diagnoses' to 'diagnos', which is not a dictionary word.\n",
        "\n",
        "*   **Lemmatization:** Lemmatization, on the other hand, is a more sophisticated process that uses vocabulary and a morphological analysis of words to return the base or dictionary form of a word, known as a 'lemma'. It considers the word's part of speech and performs a linguistic analysis to ensure the root form is a valid word. For instance, it correctly reduces 'diagnosed' and 'diagnoses' to the lemma 'diagnose'.\n",
        "\n",
        "**2. Key differences in their outputs (referencing the examples):**\n",
        "The comparison table clearly illustrates the differences:\n",
        "\n",
        "| Original     | Stemmed   | Lemmatized |\n",
        "|:-------------|:----------|:-----------|\n",
        "| diagnosed    | diagnos   | diagnose   |\n",
        "| diagnoses    | diagnos   | diagnose   |\n",
        "| imaging      | imag      | image      |\n",
        "| images       | imag      | image      |\n",
        "| treatments   | treatment | treatment  |\n",
        "| treated      | treat     | treat      |\n",
        "| recovery     | recoveri  | recovery   |\n",
        "| recovered    | recov     | recover    |\n",
        "| antibiotics  | antibiot  | antibiotic |\n",
        "| antibiotic   | antibiot  | antibiotic |\n",
        "| cells        | cell      | cell       |\n",
        "| cell         | cell      | cell       |\n",
        "| presented    | present   | present    |\n",
        "| presenting   | present   | present    |\n",
        "| localization | local     | localization |\n",
        "| localized    | local     | localize   |\n",
        "\n",
        "*   **Valid Word Forms:** Stemming often produces non-dictionary words (e.g., 'diagnos' from 'diagnosed'/'diagnoses', 'imag' from 'imaging'/'images', 'recoveri' from 'recovery'). Lemmatization consistently yields valid dictionary words (e.g., 'diagnose', 'image', 'recovery').\n",
        "*   **Preservation of Meaning:** Lemmatization aims to preserve the semantic meaning by returning the true base form. For example, 'localization' is a valid noun, and spaCy correctly lemmatizes it to 'localization', while stemming reduces it to 'local', which changes its grammatical category and potentially its precise meaning in a medical context. Similarly, 'recovered' lemmatizes to 'recover' (a verb), while stemming gives 'recov'.\n",
        "*   **Contextual Understanding:** Lemmatizers like spaCy's leverage part-of-speech tagging and contextual information, which is why 'localization' is not reduced to 'local' (an adjective) but kept as the noun 'localization'. Stemmers do not have this linguistic intelligence.\n",
        "\n",
        "**3. Why lemmatization produces more accurate and meaningful results than stemming, especially in healthcare NLP:**\n",
        "\n",
        "Lemmatization is generally preferred over stemming in healthcare NLP due to its ability to preserve the semantic integrity and grammatical correctness of words. This is paramount in a domain where precision and clarity are critical.\n",
        "\n",
        "*   **Accuracy:** By returning a valid base form, lemmatization reduces ambiguity and maintains the exact meaning of medical terms. Stemming, with its aggressive truncation, can sometimes conflate words with different meanings or create uninterpretable roots.\n",
        "*   **Meaning Preservation:** In healthcare, subtle differences in word forms can carry significant clinical implications. For instance, 'infect' (verb), 'infection' (noun), and 'infectious' (adjective) all relate to the same concept but have distinct uses and meanings. A lemmatizer will typically map them to their correct lemmas while a stemmer might reduce them to a common, less informative stem like 'infect'.\n",
        "\n",
        "**4. Specific examples of how the nuances of medical language make lemmatization crucial:**\n",
        "\n",
        "*   **Precise Terminology:** Medical language is highly precise. Terms like 'cardiac' vs. 'cardiology' vs. 'cardiovascular' are related but distinct. A good lemmatizer will differentiate these or correctly group inflected forms to their appropriate lemma, maintaining clinical accuracy. Stemming might reduce them to an ambiguous 'cardiac' or even 'card'.\n",
        "*   **Patient Safety:** Misinterpretation of patient records due to incorrect word processing can have severe consequences. If a system confuses 'diagnosed' with 'diagnosis' or 'treating' with 'treatment' in a way that loses the original grammatical role, it could lead to errors in clinical decision support systems or information extraction. Lemmatization ensures that the base form retains its intended meaning, reducing the risk of such misunderstandings.\n",
        "*   **Clinical Decision Support (CDS):** In CDS systems, accurately identifying concepts from clinical notes is vital. If a system is looking for all mentions of 'treatment' and 'treated', lemmatization ensures that both forms are correctly mapped to 'treat', thus providing a comprehensive view without conflating them with unrelated concepts that might share a similar stem but different meaning.\n",
        "*   **Information Extraction (IE):** For tasks like extracting symptoms, diagnoses, or medications from unstructured text, lemmatization helps in standardizing variations of medical terms. For example, if a system needs to identify all mentions of a drug, 'administering', 'administered', and 'administration' can all be linked to the base form 'administer', ensuring complete and accurate data extraction.\n",
        "*   **Patient Record Analysis:** When analyzing large volumes of patient records for research or epidemiological studies, consistency in word representation is key. Lemmatization helps in aggregating data around canonical medical terms, leading to more reliable statistical analyses and insights.\n",
        "\n",
        "In summary, while stemming is simpler and faster, its aggressive nature often sacrifices precision. Lemmatization, though computationally more intensive, provides a linguistically sound base form, which is indispensable in fields like healthcare NLP where accuracy, semantic integrity, and avoidance of ambiguity are paramount for effective and safe applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88207c98"
      },
      "source": [
        "### Comparison of Stemming and Lemmatization and their Importance in Healthcare NLP\n",
        "\n",
        "**1. What each technique does:**\n",
        "\n",
        "*   **Stemming:** Stemming is a heuristic process that chops off the ends of words in the hope of achieving a common base form, often called a 'stem'. It typically removes suffixes to reduce words to a common root form, but this root form is not necessarily a valid word. For example, the Porter Stemmer might reduce 'diagnosed' and 'diagnoses' to 'diagnos', which is not a dictionary word.\n",
        "\n",
        "*   **Lemmatization:** Lemmatization, on the other hand, is a more sophisticated process that uses vocabulary and a morphological analysis of words to return the base or dictionary form of a word, known as a 'lemma'. It considers the word's part of speech and performs a linguistic analysis to ensure the root form is a valid word. For instance, it correctly reduces 'diagnosed' and 'diagnoses' to the lemma 'diagnose'.\n",
        "\n",
        "**2. Key differences in their outputs (referencing the examples):**\n",
        "The comparison table clearly illustrates the differences:\n",
        "\n",
        "| Original     | Stemmed   | Lemmatized |\n",
        "|:-------------|:----------|:-----------|\n",
        "| diagnosed    | diagnos   | diagnose   |\n",
        "| diagnoses    | diagnos   | diagnose   |\n",
        "| imaging      | imag      | image      |\n",
        "| images       | imag      | image      |\n",
        "| treatments   | treatment | treatment  |\n",
        "| treated      | treat     | treat      |\n",
        "| recovery     | recoveri  | recovery   |\n",
        "| recovered    | recov     | recover    |\n",
        "| antibiotics  | antibiot  | antibiotic |\n",
        "| antibiotic   | antibiot  | antibiotic |\n",
        "| cells        | cell      | cell       |\n",
        "| cell         | cell      | cell       |\n",
        "| presented    | present   | present    |\n",
        "| presenting   | present   | present    |\n",
        "| localization | local     | localization |\n",
        "| localized    | local     | localize   |\n",
        "\n",
        "*   **Valid Word Forms:** Stemming often produces non-dictionary words (e.g., 'diagnos' from 'diagnosed'/'diagnoses', 'imag' from 'imaging'/'images', 'recoveri' from 'recovery'). Lemmatization consistently yields valid dictionary words (e.g., 'diagnose', 'image', 'recovery').\n",
        "*   **Preservation of Meaning:** Lemmatization aims to preserve the semantic meaning by returning the true base form. For example, 'localization' is a valid noun, and spaCy correctly lemmatizes it to 'localization', while stemming reduces it to 'local', which changes its grammatical category and potentially its precise meaning in a medical context. Similarly, 'recovered' lemmatizes to 'recover' (a verb), while stemming gives 'recov'.\n",
        "*   **Contextual Understanding:** Lemmatizers like spaCy's leverage part-of-speech tagging and contextual information, which is why 'localization' is not reduced to 'local' (an adjective) but kept as the noun 'localization'. Stemmers do not have this linguistic intelligence.\n",
        "\n",
        "**3. Why lemmatization produces more accurate and meaningful results than stemming, especially in healthcare NLP:**\n",
        "\n",
        "Lemmatization is generally preferred over stemming in healthcare NLP due to its ability to preserve the semantic integrity and grammatical correctness of words. This is paramount in a domain where precision and clarity are critical.\n",
        "\n",
        "*   **Accuracy:** By returning a valid base form, lemmatization reduces ambiguity and maintains the exact meaning of medical terms. Stemming, with its aggressive truncation, can sometimes conflate words with different meanings or create uninterpretable roots.\n",
        "*   **Meaning Preservation:** In healthcare, subtle differences in word forms can carry significant clinical implications. For instance, 'infect' (verb), 'infection' (noun), and 'infectious' (adjective) all relate to the same concept but have distinct uses and meanings. A lemmatizer will typically map them to their correct lemmas while a stemmer might reduce them to a common, less informative stem like 'infect'.\n",
        "\n",
        "**4. Specific examples of how the nuances of medical language make lemmatization crucial:**\n",
        "\n",
        "*   **Precise Terminology:** Medical language is highly precise. Terms like 'cardiac' vs. 'cardiology' vs. 'cardiovascular' are related but distinct. A good lemmatizer will differentiate these or correctly group inflected forms to their appropriate lemma, maintaining clinical accuracy. Stemming might reduce them to an ambiguous 'cardiac' or even 'card'.\n",
        "*   **Patient Safety:** Misinterpretation of patient records due to incorrect word processing can have severe consequences. If a system confuses 'diagnosed' with 'diagnosis' or 'treating' with 'treatment' in a way that loses the original grammatical role, it could lead to errors in clinical decision support systems or information extraction. Lemmatization ensures that the base form retains its intended meaning, reducing the risk of such misunderstandings.\n",
        "*   **Clinical Decision Support (CDS):** In CDS systems, accurately identifying concepts from clinical notes is vital. If a system is looking for all mentions of 'treatment' and 'treated', lemmatization ensures that both forms are correctly mapped to 'treat', thus providing a comprehensive view without conflating them with unrelated concepts that might share a similar stem but different meaning.\n",
        "*   **Information Extraction (IE):** For tasks like extracting symptoms, diagnoses, or medications from unstructured text, lemmatization helps in standardizing variations of medical terms. For example, if a system needs to identify all mentions of a drug, 'administering', 'administered', and 'administration' can all be linked to the base form 'administer', ensuring complete and accurate data extraction.\n",
        "*   **Patient Record Analysis:** When analyzing large volumes of patient records for research or epidemiological studies, consistency in word representation is key. Lemmatization helps in aggregating data around canonical medical terms, leading to more reliable statistical analyses and insights.\n",
        "\n",
        "In summary, while stemming is simpler and faster, its aggressive nature often sacrifices precision. Lemmatization, though computationally more intensive, provides a linguistically sound base form, which is indispensable in fields like healthcare NLP where accuracy, semantic integrity, and avoidance of ambiguity are paramount for effective and safe applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcc1aa2d"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the preprocessing steps, the differences observed between stemming and lemmatization, and reiterate the importance of lemmatization for accurate analysis in healthcare NLP.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6f98288"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Text Corpus Loading and Tokenization**: A medical text corpus was successfully loaded. Both NLTK and spaCy were used for tokenization, yielding 12 sentences and an average of 173 words respectively. NLTK required downloading the `punkt` tokenizer for sentence tokenization.\n",
        "*   **Stemming Process**: NLTK's `PorterStemmer` was applied to 95 unique words. Stemming often resulted in truncated, non-dictionary forms, such as 'diagnosed' and 'diagnoses' both becoming 'diagnos', 'imaging' and 'images' becoming 'imag', and 'recovery' becoming 'recoveri'.\n",
        "*   **Lemmatization Process**: spaCy's lemmatizer was applied to 106 unique words. Lemmatization consistently produced valid dictionary words (lemmas), for instance, 'diagnosed' and 'diagnoses' both lemmatized to 'diagnose', 'imaging' and 'images' to 'image', and 'recovery' to 'recovery'.\n",
        "*   **Differences Between Stemming and Lemmatization**: Stemming is a heuristic process that aggressively chops off word endings, frequently producing non-valid, truncated roots. In contrast, lemmatization uses morphological analysis and vocabulary to return the true base or dictionary form of a word, ensuring the output is a valid word and often preserving its part of speech and meaning.\n",
        "*   **Importance of Lemmatization in Healthcare NLP**: Lemmatization is crucial for healthcare NLP due to its ability to preserve semantic integrity and grammatical correctness. This is vital for:\n",
        "    *   **Accuracy**: Reducing ambiguity and maintaining the exact meaning of medical terms (e.g., distinguishing 'localization' from 'local').\n",
        "    *   **Patient Safety**: Avoiding misinterpretation in patient records that could lead to clinical errors.\n",
        "    *   **Clinical Decision Support (CDS)** and **Information Extraction (IE)**: Ensuring comprehensive and precise identification of medical concepts and terms (e.g., mapping 'administering', 'administered', 'administration' to 'administer').\n",
        "    *   **Patient Record Analysis**: Providing consistent word representation for reliable data aggregation and statistical analysis.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The preference for lemmatization over stemming in healthcare NLP is critical; while stemming is faster, its aggressive nature compromises precision and can lead to loss of vital clinical meaning, whereas lemmatization maintains accuracy and semantic integrity essential for sensitive medical data processing.\n",
        "*   For future healthcare NLP projects, prioritize lemmatization to ensure the highest possible accuracy and interpretability of linguistic analysis, especially when developing systems for clinical decision support, information extraction, or patient record analysis where precision directly impacts patient outcomes.\n"
      ]
    }
  ]
}